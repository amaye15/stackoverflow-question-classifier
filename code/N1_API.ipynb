{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Définir la stratégie de collecte de données\n",
    "En recensant les API disponibles, et réaliser la collecte des données répondant à des critères définis via une API (interface de programmation) en prenant en compte les normes RGPD, afin de les exploiter pour l’élaboration d’un modèle.\n",
    "\n",
    "### CE1: Stratégie de Collecte de Données\n",
    "Vous avez défini sa stratégie de collecte de données et recensé les API disponibles pour le besoin de votre projet.\n",
    "\n",
    "### CE2: Requête de l'API\n",
    "Vous avez écrit et testé une requête pour obtenir les données via l’API.\n",
    "\n",
    "### CE3: Sélection des Champs Nécessaires\n",
    "Vous avez récupéré les seuls champs nécessaires. Dans le cadre de ce projet, il s’agit par exemple :\n",
    "- Des champs qui ont servi de filtre dans la requête SQL.\n",
    "- Des champs utiles au traitement de proposition de mots clés : `Title`, `Body`, `Tags`.\n",
    "\n",
    "### CE4: Application des Filtres\n",
    "Vous avez appliqué au moins un filtre sur un des champs nécessaires pour ne collecter que les données ayant les valeurs correspondantes sur ce ou ces champs.\n",
    "\n",
    "### CE5: Stockage des Données\n",
    "Vous avez stocké les données collectées via l’API dans un fichier utilisable (ex. : fichier CSV ou pickle).\n",
    "\n",
    "### CE6: Respect des Normes RGPD\n",
    "Vous avez veillé au respect des normes RGPD dans toutes les phases de la collecte et du stockage des données. En particulier, il présente les 5 grands principes du RGPD et sa mise en oeuvre sur le projet :\n",
    "- Il ne gère que les données nécessaires pour la finalité du projet.\n",
    "- Il ne gère aucune information permettant d’identifier les personnes qui sont les auteurs des questions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from stackapi import StackAPI\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from datasets import Dataset\n",
    "\n",
    "data_repository = \"amaye15/Stack-Overflow-Zero-Shot-Classification\"\n",
    "\n",
    "# Initialize key\n",
    "stack_key = \"ub*oRqta6kWgck7l2tG5ng((\"\n",
    "hf_key = \"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\"\n",
    "\n",
    "# Initialize StackAPI\n",
    "SITE = StackAPI('stackoverflow', key = stack_key)\n",
    "\n",
    "# Define Query Parameters\n",
    "tag = None\n",
    "min_score = 50\n",
    "from_date = datetime.strptime('2010-01-01', '%Y-%m-%d').timestamp()\n",
    "to_date = datetime.strptime('2023-01-31', '%Y-%m-%d').timestamp()\n",
    "\n",
    "# Set the max pagesize\n",
    "max_pagesize = 100\n",
    "\n",
    "# Initialize list to store question data\n",
    "questions_data = []\n",
    "\n",
    "# Fetch Questions with pagination up to page 25\n",
    "is_fetching = True\n",
    "page = 1\n",
    "max_page = 25  # Set the maximum page to 25\n",
    "\n",
    "# Collect Results\n",
    "while is_fetching:\n",
    "    questions = SITE.fetch('questions', \n",
    "                           tagged=tag, # All tags\n",
    "                           min=min_score, # Good Questions Only\n",
    "                           fromdate=int(from_date),\n",
    "                           todate=int(to_date), \n",
    "                           sort='votes', \n",
    "                           order='desc', \n",
    "                           pagesize=max_pagesize, \n",
    "                           page=page,)\n",
    "    \n",
    "    for question in questions['items']:\n",
    "        title = question['title']\n",
    "        tags = ', '.join(question['tags'])\n",
    "        questions_data.append({'Title': title, 'Tags': tags})\n",
    "    \n",
    "    page += 1\n",
    "    is_fetching = 'has_more' in questions and questions['has_more']\n",
    "\n",
    "# Create DataFrame from list\n",
    "df = pd.DataFrame(questions_data, columns=['Title', 'Tags']).drop_duplicates()\n",
    "\n",
    "# Convert Dataframe to Huggingface Dataset\n",
    "ds = Dataset.from_pandas(df)\n",
    "\n",
    "# Push Dataset to Huggingface -> Parquet File\n",
    "ds.push_to_hub(data_repository, token = hf_key)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
